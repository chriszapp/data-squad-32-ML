{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yaxTV5iyhQ82"
   },
   "source": [
    "# ML Tech Interview\n",
    "\n",
    "Hello and welcome to the Machine Learning Tech Interview. This interview will be divided in two parts: the theoretical part and the practical/coding part. \n",
    "\n",
    "### **I will review only the scripts that will be sent (by pull request on this repo) by 5:00 pm on Friday**\n",
    "\n",
    "Good Luck!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQjWqn__hQ85"
   },
   "source": [
    "## Theoretical Part\n",
    "\n",
    "Please answer the following questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0X1MRGNhQ86"
   },
   "source": [
    "#### What are the assumptions of a linear model (or any other type of model)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9rLLjNphQ86"
   },
   "source": [
    "* IID\n",
    "\n",
    "Two basic assumptions underlying most of machine learning are that the available examples are independent and identically distributed (IID), according to an unknown probability distribution. If you think about the algorithms that you know, both of these are pretty obvious. Models are trained on a set and tested on another, under the assumption that the two are very strongly correlated (once overfitting is taken care of). Similarly, most algorithms do not care about the order in which data is presented (in technical jargon, they are symmetric), and all data is treated the same.\n",
    "\n",
    "* Sample sizes\n",
    "\n",
    "Samples that are too large can cause us to overfit our model. Having too many samples can cause variables that are actually insignificant have statistical significance in an analysis. However, taking a sample that is too small from our dataset may also cause problems. An analysis conducted on too small a sample will lack statistical power, which is essential in being able to make accurate predictions based on the model. \n",
    "\n",
    "\n",
    "* Outliers\n",
    "\n",
    "There are no specific assumptions about outliers in model creation, but it is important to note that outliers can greatly influence your model and alter its effectiveness.\n",
    "\n",
    "* Bias\n",
    "\n",
    "Another set of assumptions stems from all the biases that you (implicitly or explicitly) select when designing a machine learning system. Some of these derive from choosing a particular model with respect to another (model bias), e.g. a linear classifier assumes that the decision boundaries are linear, or a neural network with one hidden layer and a fixed number of neurons assumes that these have enough complexity to model the underlying function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWNx7P4HhQ8_"
   },
   "source": [
    "#### What’s the difference between K Nearest Neighbor and K-means Clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQ16LsGthQ8_"
   },
   "source": [
    "1. K-NN is a Supervised machine learning while K-means is an unsupervised machine learning.\n",
    "2. K-NN is a classification or regression machine learning algorithm while K-means is a clustering machine learning algorithm.\n",
    "3. K-NN is a lazy learner while K-Means is an eager learner. An eager learner has a model fitting that means a training step but a lazy learner does not have a training phase.\n",
    "4. K-NN performs much better if all of the data have the same scale but this is not true for K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDDPRfBRhQ9B"
   },
   "source": [
    "#### How do you address overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1TyBsLwhQ9D"
   },
   "source": [
    "In the case that we cannot find more data to add to our dataset, there are some techniques we can apply:\n",
    "\n",
    "1. Data augmentation to artificially add sufficiently similar datapoints to our dataset. (Apply in training data only)\n",
    "\n",
    "2. We can reduce the number of features we choose to include in our model, by excluding those features less related to the outcome, and therefore reducing the amount of noise to be modeled the dataset. \n",
    "\n",
    "3. Dimensionality reduction technique, where the algorithm starts by locating the underlying trends in our features. Next, the algorithm estimates and rewrites every other feature in terms of their exposures to these underlying trends.\n",
    "\n",
    "4. Principal Component Analysis (PCA). This method combines highly correlated variables together to form a smaller number of an artificial set of variables which is called “principal components” that account for most variance in the data.\n",
    "\n",
    "5. I know that regularization is also a technique used to simplify the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dInW-RQhQ9G"
   },
   "source": [
    "#### Explain Naive Bayes algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U76IQ8xdhQ9H"
   },
   "source": [
    "Naive Bayes algorithms utilize the most fundamental probability knowledge and make a naive assumption that all features are independent. They are probabilistic algorithms that are typically used for classification problems with an assumption of independence among predictors.\n",
    "\n",
    "For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "Naive Bayes algorithms are mostly used in sentiment analysis, spam filtering, recommendation systems etc. They are fast and easy to implement but their biggest disadvantage is that the requirement of predictors to be independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UsLyyonHYwN"
   },
   "source": [
    "#### When do you use an AUC-ROC score? What kind of information can you gather from it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jp5SotUdHrZf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gMzc684hQ9J"
   },
   "source": [
    "#### What is cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yhBiZyJhQ9K"
   },
   "source": [
    "Cross validation (CV) is one of the technique used to test the effectiveness of a machine learning models, it is also a re-sampling procedure used to evaluate a model if we have a limited data.You can either use the typical train/test split or use a cross validation method like K-fold.\n",
    "\n",
    "* How does cross-validation works?\n",
    "\n",
    "First, we split the entire data randomly into n folds (value of n shouldn’t be too small or too high, ideally we choose 5 to 10 depending on the data size). The higher value of n leads to less biased model (but large variance might lead to overfit), where as the lower value of n is similar to the train-test split approach we saw before.\n",
    "Then, we fit the model using the n - 1 folds and validate the model using the remaining nth fold.\n",
    "Repeat this process until every n-fold serve as the test set. Then we calculate the average of the recorded scores and that will be the performance metric for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqEQlGL2hQ9L"
   },
   "source": [
    "#### What are confounding variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-8POEd6hQ9M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ixSZaNdhQ9P"
   },
   "source": [
    "#### If an important metric for our company stopped appearing in our data source, how would you investigate the causes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOPUF1i3hQ9Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ko6vUaXshQ9S"
   },
   "source": [
    "## Practical Machine Learning\n",
    "\n",
    "In this challenge, you will showcase your knowledge in feature engineering, dimensionality reduction, model selection and evaluation, hyperparameter tuning, and any other techniques of machine learning.\n",
    "\n",
    "There isn't a correct solution to this challenge. All we would like to learn is your thinking process that demonstrates your knowledge, experience, and creativity in developing machine learning models. Therefore, in addition to developing the model and optimizing its performance, you should also elaborate your thinking process and justify your decisions thoughout the iterative problem-solving process.\n",
    "\n",
    "The suggested time to spend on this challenge is 90-120 minutes. If you don't have time to finish all the tasks you plan to do, simply document the to-dos at the end of your response.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Download the housing prices data set (housing_prices.csv). The data is big enough to showcase your thoughts but not so that processing power will be a problem.\n",
    "- Using Python, analyze the features and determine which feature set to select for modeling.\n",
    "- Train and cross validate several regression models, attempting to accurately predict the SalePrice target variable.\n",
    "- Evaluate all models and show comparison of performance metrics.\n",
    "- State your thoughts on model performance, which model(s) you would select, and why.\n",
    "\n",
    "#### Deliverables Checklist:\n",
    "\n",
    "- Python code.\n",
    "- Your thinking process.\n",
    "- The features selected for machine learning.\n",
    "- The results (e.g., performance metrics) of your selected model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3moKc9n4hQ9T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLtechInterview.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
